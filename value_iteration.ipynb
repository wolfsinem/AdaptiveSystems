{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38064bit94409e1e4df94da1b5cc700cc0e6ab29",
   "display_name": "Python 3.8.0 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the maze with all rewards on each cell\n",
    "SMALL_ENOUGH = 0.005\n",
    "GAMMA = 0.7         \n",
    "NOISE = 0.10  \n",
    "\n",
    "DIRECTION = {\n",
    "    3: '\\u2191', #U\n",
    "    2: '\\u2192', #R\n",
    "    1: '\\u2193', #D\n",
    "    0: '\\u2190' #L\n",
    "}\n",
    "# print(' '.join([ACTION[i] for i in range(4)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maze_states(cols,rows):\n",
    "    \"\"\"Initialize all states for given grid size.\n",
    "    \"\"\"\n",
    "    all_states = []\n",
    "    for i in range(cols):\n",
    "        for j in range(rows):\n",
    "                all_states.append((i,j))\n",
    "    return all_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (2, 0),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (2, 3),\n",
       " (3, 0),\n",
       " (3, 1),\n",
       " (3, 2),\n",
       " (3, 3)]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "states = maze_states(4,4)\n",
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each state which I printed above has their own reward, these are defined here\n",
    "rewards = {}\n",
    "for i in states:\n",
    "    if i == (0,0):\n",
    "        rewards[i] = +10\n",
    "    elif i == (0,1):\n",
    "        rewards[i] = -2\n",
    "    elif i == (2,2):\n",
    "        rewards[i] = -10\n",
    "    elif i == (2,3):\n",
    "        rewards[i] = -10\n",
    "    elif i == (3,3):\n",
    "        rewards[i] = +40\n",
    "    else:\n",
    "        rewards[i] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{(0, 0): 10,\n",
       " (0, 1): -2,\n",
       " (0, 2): -1,\n",
       " (0, 3): -1,\n",
       " (1, 0): -1,\n",
       " (1, 1): -1,\n",
       " (1, 2): -1,\n",
       " (1, 3): -1,\n",
       " (2, 0): -1,\n",
       " (2, 1): -1,\n",
       " (2, 2): -10,\n",
       " (2, 3): -10,\n",
       " (3, 0): -1,\n",
       " (3, 1): -1,\n",
       " (3, 2): -1,\n",
       " (3, 3): 40}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All possible directions at a given state. For example, you're only able to go Up and Left at state 0,0\n",
    "ACTIONS = {\n",
    " (0, 0): ('U','L'),\n",
    " (0, 1):('U','R','L'),\n",
    " (0, 2):('U','L','R'),\n",
    " (0, 3):('L','U'),\n",
    " (1, 0):('U','D','R'),\n",
    " (1, 1):('U','D','L','R'),\n",
    " (1, 2):('U','D','L','R'),\n",
    " (1, 3):('U','D','L'),\n",
    " (2, 0):('U','D','R'),\n",
    " (2, 1):('U','D','L','R'),\n",
    " (2, 2):('U','D','L','R'),\n",
    " (2, 3):('U','D','L'),\n",
    " (3, 0):('D','R'),\n",
    " (3, 1):('D','L','R'),\n",
    " (3, 2):('D','L','R'),\n",
    " (3, 3):('D','L')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The initial policy is defined here based on all the actions above\n",
    "policy = {}\n",
    "for s in ACTIONS.keys():\n",
    "    policy[s] = np.random.choice(ACTIONS[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{(0, 0): 'U',\n",
       " (0, 1): 'R',\n",
       " (0, 2): 'R',\n",
       " (0, 3): 'L',\n",
       " (1, 0): 'R',\n",
       " (1, 1): 'R',\n",
       " (1, 2): 'L',\n",
       " (1, 3): 'L',\n",
       " (2, 0): 'R',\n",
       " (2, 1): 'D',\n",
       " (2, 2): 'U',\n",
       " (2, 3): 'U',\n",
       " (3, 0): 'D',\n",
       " (3, 1): 'R',\n",
       " (3, 2): 'D',\n",
       " (3, 3): 'L'}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "policy # This is an example policy/route the agent can take "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define initial value function\n",
    "V = {}\n",
    "for s in states:\n",
    "    if s in ACTIONS.keys():\n",
    "        V[s] = -1\n",
    "    if s == (0,0):\n",
    "        V[s] = +10\n",
    "    if s == (0,1):\n",
    "        V[s] = -2\n",
    "    if s == (2,2):\n",
    "        V[s] = -10\n",
    "    if s == (2,3):\n",
    "        V[s] = -10\n",
    "    if s == (3,3):\n",
    "        V[s] = +40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{(0, 0): 10,\n",
       " (0, 1): -2,\n",
       " (0, 2): -1,\n",
       " (0, 3): -1,\n",
       " (1, 0): -1,\n",
       " (1, 1): -1,\n",
       " (1, 2): -1,\n",
       " (1, 3): -1,\n",
       " (2, 0): -1,\n",
       " (2, 1): -1,\n",
       " (2, 2): -10,\n",
       " (2, 3): -10,\n",
       " (3, 0): -1,\n",
       " (3, 1): -1,\n",
       " (3, 2): -1,\n",
       " (3, 3): 40}"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(-1, 0)\n(0, -1)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "(-1, 0)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d89bd9f2b4c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mGAMMA\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mNOISE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnxt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNOISE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnew_v\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#Is this the best action so far? If so, keep it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0mnew_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (-1, 0)"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "\n",
    "while True:\n",
    "    biggest_change = 0\n",
    "    for s in states:            \n",
    "        if s in policy:\n",
    "            \n",
    "            old_v = V[s]\n",
    "            new_v = 0\n",
    "            \n",
    "            for a in ACTIONS[s]:\n",
    "                if a == 'U':\n",
    "                    nxt = [s[0]-1, s[1]]\n",
    "                if a == 'D':\n",
    "                    nxt = [s[0]+1, s[1]]\n",
    "                if a == 'L':\n",
    "                    nxt = [s[0], s[1]-1]\n",
    "                if a == 'R':\n",
    "                    nxt = [s[0], s[1]+1]\n",
    "\n",
    "                #Choose a new random action to do (transition probability)\n",
    "                random_1=np.random.choice([i for i in ACTIONS[s] if i != a])\n",
    "                if random_1 == 'U':\n",
    "                    act = [s[0]-1, s[1]]\n",
    "                if random_1 == 'D':\n",
    "                    act = [s[0]+1, s[1]]\n",
    "                if random_1 == 'L':\n",
    "                    act = [s[0], s[1]-1]\n",
    "                if random_1 == 'R':\n",
    "                    act = [s[0], s[1]+1]\n",
    "\n",
    "                #Calculate the value\n",
    "                nxt = tuple(nxt)\n",
    "                act = tuple(act)\n",
    "                print(nxt)\n",
    "                print(act)\n",
    "                v = rewards[s] + (GAMMA * ((1-NOISE)* V[nxt] + (NOISE * V[act]))) \n",
    "                if v > new_v: #Is this the best action so far? If so, keep it\n",
    "                    new_v = v\n",
    "                    policy[s] = a\n",
    "\n",
    "       #Save the best of all actions for the state                                \n",
    "            V[s] = new_v\n",
    "            biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n"
   ]
  }
 ]
}