{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python271564bit2d80c6eba8a6433f8fc10cb221be4ed7",
   "display_name": "Python 2.7.15 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Model-Free Prediction and Control"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Import libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = \"Courier New\"\n",
    "import seaborn as sns\n",
    "import copy\n",
    "\n",
    "from maze_class import *\n",
    "from monte_carlo import *\n",
    "from td_learning import *"
   ]
  },
  {
   "source": [
    "## Monte Carlo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Initialise parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EP = 3000\n",
    "STEP_COST = -0.1\n",
    "MAX_EP_LEN = 30\n",
    "\n",
    "maze_coords = {\n",
    "    \"[0, 0]\": 0, \"[0, 1]\": 1, \"[0, 2]\": 2, \"[0, 3]\": 3,\n",
    "    \"[1, 0]\": 4, \"[1, 1]\": 5, \"[1, 2]\": 6, \"[1, 3]\": 7,\n",
    "    \"[2, 0]\": 8, \"[2, 1]\": 9, \"[2, 2]\": 10, \"[2, 3]\": 11,\n",
    "    \"[3, 0]\": 12, \"[3, 1]\": 13, \"[3, 2]\": 14, \"[3, 3]\": 15\n",
    "}\n",
    "\n",
    "reversed_maze = {\n",
    "    \"0\": [0, 0], \"1\": [0, 1], \"2\": [0, 2], \"3\": [0, 3],\n",
    "    \"4\": [1, 0], \"5\": [1, 1], \"6\": [1, 2], \"7\": [1, 3],\n",
    "    \"8\": [2, 0], \"9\": [2, 1], \"10\": [2, 2], \"11\": [2, 3],\n",
    "    \"12\": [3, 0], \"13\": [3, 1], \"14\": [3, 2], \"15\": [3, 3],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze(maze_coords,reversed_maze, step_cost=STEP_COST, max_ep_length=MAX_EP_LEN)"
   ]
  },
  {
   "source": [
    "#### Visualize rewards with function below"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_state_value_grid(state_value_grid):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,5))\n",
    "    p=sns.heatmap(state_value_grid, cmap='coolwarm', annot=True, fmt=\".1f\",annot_kws={'size':16},square=True)\n",
    "    p.set_ylim(len(state_value_grid)+0.01, -0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factor 1\n",
    "policy_eval1 = mc_evaluation_policy(env, discount_factor=1)\n",
    "\n",
    "# factor 0.99\n",
    "policy_eval2 = mc_evaluation_policy(env, discount_factor=0.9)"
   ]
  },
  {
   "source": [
    "#### Factor 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state_value_grid(policy_eval1.reshape((4,4)))\n"
   ]
  },
  {
   "source": [
    "#### Factor 0.9"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state_value_grid(policy_eval2.reshape((4,4)))"
   ]
  },
  {
   "source": [
    "## TD Learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factor 1\n",
    "policy_eval1 = td_learning(env, discount_factor=1)\n",
    "\n",
    "# factor 0.99\n",
    "policy_eval2 = td_learning(env, discount_factor=0.9)"
   ]
  },
  {
   "source": [
    "#### Factor 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state_value_grid(policy_eval1.reshape((4,4)))"
   ]
  },
  {
   "source": [
    "#### Factor 0.9"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state_value_grid(policy_eval2.reshape((4,4)))"
   ]
  },
  {
   "source": [
    "## On-policy first-visit Monte-Carlo Control"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Initialize rewards lists "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_lists(cells, actions):\n",
    "    # Each state has four possible actions to take\n",
    "    def create_array(n, lst):\n",
    "        for i in range(n):\n",
    "            lst.append(str(i))\n",
    "        return lst \n",
    "\n",
    "    possible_states = []\n",
    "    possible_states = create_array(cells, possible_states)\n",
    "\n",
    "    possible_actions = []\n",
    "    possible_actions = create_array(actions, possible_actions)\n",
    "\n",
    "    rewards = {}\n",
    "    for state in possible_states:\n",
    "        for action in possible_actions:\n",
    "            rewards[state+\", \"+action] = []\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze(maze_coords,reversed_maze, step_cost=STEP_COST, max_ep_length=MAX_EP_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_learning(env, discount_factor):\n",
    "    maze_rewards = np.zeros((16,4))\n",
    "\n",
    "    maze_rewards[3] = 40\n",
    "    maze_rewards[12] = 10\n",
    "    maze_rewards[13] = -2\n",
    "    maze_rewards[6] = -10\n",
    "    maze_rewards[7] = -10\n",
    "\n",
    "    rewards = rewards_lists(16,4)\n",
    "\n",
    "    for ep in range(MAX_EP):\n",
    "        G = 0\n",
    "        state = env.reset()\n",
    "        trajectory = []\n",
    "        while True:\n",
    "            action_values = maze_rewards[state]\n",
    "            probs = probability(action_values)\n",
    "            action = np.random.choice(np.arange(4),p=probs) \n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "            trajectory.append((state, action, reward))\n",
    "            \n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        for idx, step in enumerate(trajectory[::-1]):\n",
    "            G = discount_factor * G + step[2]\n",
    "            # first visit check\n",
    "            if step[0] not in np.array(trajectory[::-1])[:,0][idx+1:]:\n",
    "                rewards[str(step[0])+\", \"+str(step[1])].append(G)\n",
    "                maze_rewards[step[0]][step[1]] = np.mean(rewards[str(step[0])+\", \"+str(step[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_values = td_learning(env,discount_factor=0.9)\n",
    "print(grid_values)"
   ]
  },
  {
   "source": [
    "#### Factor 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quatromatrix(left, bottom, right, top, ax=None, triplotkw={},tripcolorkw={}):\n",
    "\n",
    "    if not ax: ax=plt.gca()\n",
    "    n = left.shape[0]; m=left.shape[1]\n",
    "\n",
    "    a = np.array([[0,0],[0,1],[.5,.5],[1,0],[1,1]])\n",
    "    tr = np.array([[0,1,2], [0,2,3],[2,3,4],[1,2,4]])\n",
    "\n",
    "    A = np.zeros((n*m*5,2))\n",
    "    Tr = np.zeros((n*m*4,3))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            k = i*m+j\n",
    "            A[k*5:(k+1)*5,:] = np.c_[a[:,0]+j, a[:,1]+i]\n",
    "            Tr[k*4:(k+1)*4,:] = tr + k*5\n",
    "\n",
    "    C = np.c_[ left.flatten(), bottom.flatten(), \n",
    "              right.flatten(), top.flatten()   ].flatten()\n",
    "\n",
    "    triplot = ax.triplot(A[:,0], A[:,1], Tr, **triplotkw)\n",
    "    tripcolor = ax.tripcolor(A[:,0], A[:,1], Tr, facecolors=C, **tripcolorkw)\n",
    "    return tripcolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_action_value(action_value_grid):\n",
    "    top=action_value_grid[:,0].reshape((4,4))\n",
    "    top_value_positions = [(0.38,0.25),(1.38,0.25),(2.38,0.25),(3.38,0.25),\n",
    "                           (0.38,1.25),(1.38,1.25),(2.38,1.25),(3.38,1.25),\n",
    "                           (0.38,2.25),(1.38,2.25),(2.38,2.25),(3.38,2.25),\n",
    "                           (0.38,2.25),(1.38,2.25),(2.38,2.25),(3.38,2.25)]\n",
    "\n",
    "    right=action_value_grid[:,1].reshape((4,4))\n",
    "    right_value_positions = [(0.65,0.5),(1.65,0.5),(2.65,0.5),(3.65,0.5),\n",
    "                           (0.65,1.5),(1.65,1.5),(2.65,1.5),(3.65,1.5),\n",
    "                           (0.65,2.5),(1.65,2.5),(2.65,2.5),(3.65,2.5),\n",
    "                           (0.65,2.5),(1.65,2.5),(2.65,2.5),(3.65,2.5)]\n",
    "\n",
    "    bottom=action_value_grid[:,2].reshape((4,4))\n",
    "    bottom_value_positions = [(0.38,0.8),(1.38,0.8),(2.38,0.8),(3.38,0.8),\n",
    "                           (0.38,1.8),(1.38,1.8),(2.38,1.8),(3.38,1.8),\n",
    "                           (0.38,2.8),(1.38,2.8),(2.38,2.8),(3.38,2.8),\n",
    "                           (0.38,2.8),(1.38,2.8),(2.38,2.8),(3.38,2.8)]\n",
    "\n",
    "    left=action_value_grid[:,3].reshape((4,4))\n",
    "    left_value_positions = [(0.05,0.5),(1.05,0.5),(2.05,0.5),(3.05,0.5),\n",
    "                           (0.05,1.5),(1.05,1.5),(2.05,1.5),(3.05,1.5),\n",
    "                           (0.05,2.5),(1.05,2.5),(2.05,2.5),(3.05,2.5),\n",
    "                           (0.05,2.5),(1.05,2.5),(2.05,2.5),(3.05,2.5)]\n",
    "\n",
    "    fig, ax=plt.subplots(figsize=(12,5))\n",
    "    ax.set_ylim(3, 0)\n",
    "    tripcolor = quatromatrix(left, top, right, bottom, ax=ax,\n",
    "                 triplotkw={\"color\":\"k\", \"lw\":1},\n",
    "                 tripcolorkw={\"cmap\": \"coolwarm\"}) \n",
    "\n",
    "    ax.margins(0)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    fig.colorbar(tripcolor)\n",
    "\n",
    "    for i, (xi,yi) in enumerate(top_value_positions):\n",
    "        plt.text(xi,yi,round(top.flatten()[i],2), size=11, color=\"w\")\n",
    "    for i, (xi,yi) in enumerate(right_value_positions):\n",
    "        plt.text(xi,yi,round(right.flatten()[i],2), size=11, color=\"w\")\n",
    "    for i, (xi,yi) in enumerate(left_value_positions):\n",
    "        plt.text(xi,yi,round(left.flatten()[i],2), size=11, color=\"w\")\n",
    "    for i, (xi,yi) in enumerate(bottom_value_positions):\n",
    "        plt.text(xi,yi,round(bottom.flatten()[i],2), size=11, color=\"w\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_action_value(grid_values)"
   ]
  },
  {
   "source": [
    "#### Factor 0.99"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}